{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "FakeNewsDetector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "84ed700645207affe5c965a75f2970b249aca56b75e73ca5e19eb5a42ad13de5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "lOVhahPoLkaW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import itertools\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "outputs": [],
      "metadata": {
        "id": "K8BSzab6Jzkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "eqwbrpaCLnob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = pd.read_csv('news.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "A937tiRNLpyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inspecting data"
      ],
      "metadata": {
        "id": "jKA3CblGMFGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if dataset.isnull().sum().sum():\r\n",
        "  print(\"Missing values found.\")\r\n",
        "else:\r\n",
        "  print(\"No missing entries in dataset.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No missing entries in dataset.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CbAOGZ2MH9y",
        "outputId": "a7127f01-eb72-4f26-def3-e6a350bd4bfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Text = dataset['text']\r\n",
        "Labels = dataset['label']"
      ],
      "outputs": [],
      "metadata": {
        "id": "LO6zkbidMpWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train-test-split"
      ],
      "metadata": {
        "id": "RAgHBK_mM8Oo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(Text, Labels, test_size=0.2, random_state=7)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rF7jd02fM_GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialising a TF-IDF Vectoriser"
      ],
      "metadata": {
        "id": "gJADI79KN3Ce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import  stopwords\r\n",
        "stopwordslist = set(stopwords.words('english'))#removes negative n't words from the list so they are kept in the corpus\r\n",
        "neg = set(['not'])\r\n",
        "for word in stopwordslist:\r\n",
        "    if 'n\\'t' in word:\r\n",
        "        neg.add(word)\r\n",
        "        neg.add(word[:-2])\r\n",
        "\r\n",
        "stopwordslist = stopwordslist - neg"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvJMz3vyfreL",
        "outputId": "4b2685ea-cf97-4bbf-f5ad-bc4cc278f97a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "TFIDF = TfidfVectorizer(stop_words=stopwordslist, max_df=0.7)#max_df will ignore terms with a document frequency higher than that threshold set\r\n",
        "'''\r\n",
        "look into adjusting the stop words by removing the negative ones as with the data science course.\r\n",
        "'''\r\n",
        "TFIDF_train = TFIDF.fit_transform(X_train)\r\n",
        "TFIDF_test = TFIDF.transform(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KuPHvS0gN2nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialising a PassiveAggressive Classifier"
      ],
      "metadata": {
        "id": "cTpVn1DKPLYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of a passive aggressive classifier:\n",
        "Learns on the fly from a large dataset of text/documents.\n",
        "The amount of information is too vast to store and so it learns from it then discards it, i.e. learning from tweets -> updated step-by-step rather than batch learning.\n",
        "\n",
        "Passive: If the prediction is correct, keep the model and do not make any changes. i.e., the data in the example is not enough to cause any changes in the model. \n",
        "Aggressive: If the prediction is incorrect, make changes to the model. i.e., some change to the model may correct it."
      ],
      "metadata": {
        "id": "np-_P7Q5SdtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "PAC = PassiveAggressiveClassifier(max_iter=100, early_stopping=True, n_iter_no_change=5)#max_iter is akin to maximum number of training epochs\r\n",
        "'''\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html\r\n",
        "Several parameters may improve model performance.\r\n",
        "'''\r\n",
        "PAC.fit(TFIDF_train,Y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
              "                            early_stopping=True, fit_intercept=True,\n",
              "                            loss='hinge', max_iter=100, n_iter_no_change=5,\n",
              "                            n_jobs=None, random_state=None, shuffle=True,\n",
              "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
              "                            warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiYOD4tsPPz4",
        "outputId": "2ccc31c4-5d17-4bdf-96ea-b1f99834f108"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier predictions and score"
      ],
      "metadata": {
        "id": "ysPYAJ-7RGwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "Y_pred = PAC.predict(TFIDF_test)\r\n",
        "score = accuracy_score(Y_test,Y_pred)\r\n",
        "print(f'Accuracy: {round(score*100,2)}%')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.98%\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPWAWJN0RJXq",
        "outputId": "8373fb8c-b2a7-448c-cccb-3417def9f22f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix"
      ],
      "metadata": {
        "id": "IClXl7gzSSWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "confusion_matrix(Y_test,Y_pred, labels=['FAKE','REAL'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[596,  42],\n",
              "       [ 47, 582]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRL913rASRcm",
        "outputId": "12140d8b-a2ad-44c9-d1af-9f4dd108a87a"
      }
    }
  ]
}